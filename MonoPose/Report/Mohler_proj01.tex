\documentclass[12pt]{article}
\usepackage{uwyo_report} % loads in all the specific formatting
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{float}
\usepackage{multicol}
\setlength{\multicolsep}{6.0pt plus 2.0pt minus 1.5pt}%


% if you want other packages, put them here

% macro for typesetting the word BibTeX
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
   T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}


\begin{document}

\title{EE 5450  Project 01: Simultaneous Monocular Calibration and Pose Estimation}

\author{David R. Mohler}

%\date{December 7, 1941}

\maketitle

\section{Introduction} 
Determination of the pose of rigid objects within images is a common problem within many practical application of computer vision, robotics, computer graphics, etc. The following set of experiments are focused on the application of the simultaneous monocular calibration and pose estimation algorithm to a series of images of rigid objects with known dimensions and correspondence points. From this known data, the algorithm is capable of calibrating a single monocular camera with a fixed focal length. The calibration enables simultaneous discovery of the pose of the viewed object, and from this generate a three dimensional representation of that same object.  
%In this project we will demonstrate the generation of independent and identically distributed (IID) data samples through rejection sampling of a continuous PDF. Using this data we will then apply EM to iteratively estimate the parameters of the probability distribution function. 

\section{Methods and Results}
The foundation of this project is based on the known dimensions of a given rigid object, a box for example, which is  manually assigned a number of points on the object corresponding to its corners (i.e.\ the coordinates in the object frame represent the dimensions of the object). A model of the initial box and its correspondence points can be seen in Figure \ref{box}, additionally, the measured dimensions of the object are shown in Table \ref{boxdim}. Given this data we are able to proceed with the implementation of the monocular pose algorithm. 



Using the object coordinates in their homogeneous form (i.e.\ $X_o^i = [x_o^i \quad y_o^i \quad z_o^i \quad 1]^T$), we begin with calculating an approximation of the projection matrix, $\Pi_{est}$. The projection matrix is such that $\Pi = [KR \quad KT] \in \Re^{3\times4}$, where $K$ is the camera calibration matrix, $R$ is a rotation matrix, and $T$ is the translation vector. Given that we know the location of the correspondence points, $\chi^{pj}$, and their matching locations in the object frame, $X_0^j$, assuming that a sufficient number of points are provided, we are then able to find an approximation to $\Pi$.This approximation can be expressed as a least squares minimization of Equation \ref{NPI}, where $e_3$ is the standard basis vector $[0 0 1]^T$, $\Pi^S$ is the vectorized or ``stacked'' version of $\Pi$,and $\otimes$ represents the Kronecker product. The least squares estimate of the vectorized projection matrix can be found as the minimum input direction of $N$, which is the final right singular vector yielded from the singular value decomposition (SVD) of $N$.  
\begin{equation}\label{NPI}
N^j\Pi^{pj} = [(X_o^{jT}\otimes I_3)-(X_o^{jT}\otimes \chi^{pj}e_3^T)]\Pi^S = 0
\end{equation}
\begin{figure}[h]
	\centering % must do this or your figure won't be centered
	\captionsetup{justification=centering}
	\begin{minipage}{0.5\textwidth}
		\centering % must do this or your figure won't be centered
		\includegraphics[width=0.75\textwidth]{BoxModel.png}
		\caption{Rigid object model.} \label{box}
	\end{minipage}\hfill
	\begin{minipage}{0.5\textwidth}
		\begin{center}
			\begin{tabular}[5pt]{| c| c|}
				\hline
				Parameter	& Value (cm) \\[0.5ex] 
				\hline 	
				Length& 45.6  \\ \hline 
				Height& 32.5  \\ \hline 
				Width& 10.1  \\ \hline 
			\end{tabular}
			\captionof{table}{Object Dimensions}\label{boxdim}
		\end{center}	
	\end{minipage}
\end{figure}

\noindent From this we de-vectorize the result of the SVD and obtain $\Pi_{est}$. Using this, all necessary components to describe the calibration and pose of the camera are extracted. From order reversing the standard QR decomposition we are able to obtain the scaling factor $\alpha$, the calibration matrix, and the rotation matrices corresponding to the given set of pixel coordinates. Using this information we can lastly find the translation vector associated with the system from the following equation,where $T^{'} = KT$, and is the rightmost column of $\Pi_{est}$: \nolinebreak

\begin{multicols}{3}
	
	\begin{equation}\label{Teq}
		T = \dfrac{K^{-1}T^{'}}{\alpha}
	\end{equation}\break
	\noindent
	\begin{equation}\label{xp}
		\chi^{pj} = \dfrac{\Pi_{est} X_0^j}{\lambda^j}
	\end{equation}\break
	\noindent
	\begin{equation}\label{xo}
	X_o^{pj} = R^TK^{-1}(\lambda\chi^p-KT)
	\end{equation}\break
\end{multicols}
Using the information obtained from the algorithm, we next show the ability to project estimated pixel coordinates in to the image plane. These estimated pixel coordinates are described by Equation \ref{xp}.  From this we are able to qualitatively visualize the success of the algorithm in matching provided correspondence points (Figure \ref{pixcoords}). In order to do so, we apply the algorithm to four different images of the same object (See Appendix A), all taken from seperate locations, thus enabling the ability to test the algorithm against multiple view points. From the estimated pixel coordinates in noiseless reconstructions of the object, using Equation \ref{xo}, we calculate the average root mean square error (RMSE) of the position between the true coordinates and their respective estimates in the object frame in order to establish the overall fidelity of the algorithm across multiple images taken with varying rotations, translations, and calibrations. In this case PRMSE is expressed by Equations \ref{dist} and \ref{PRMSE}, where $n_{im}$ is the number of images, and $n_{f} $ is the number of features used:
\begin{equation}\label{dist}
d_i = \sqrt{(x_o^{ij}-\hat{x}_o^{ij})+(y_o^{ij}-\hat{y}_o^{ij})+(z_o^{ij}-\hat{z}_o^{ij})}\quad \text{for} \quad j=1,2,...,n_{im}
\end{equation}
\begin{equation}\label{PRMSE}
	PRMSE =\dfrac{\sum_{j=1}^{n_{im}}\sqrt{\sum_{i=1}^{n_f}\sqrt{di/n_f}}}{n_{im}}
\end{equation}



\begin{figure}[h]
	\centering % must do this or your figure won't be centered
	\captionsetup{justification=centering}
	\begin{minipage}{0.5\textwidth}
			\centering % must do this or your figure won't be centered
		\includegraphics[width=1\textwidth]{pixcoords.eps}
		\caption{Estimated pixel coordinates} \label{pixcoords} 
	\end{minipage}\hfill
	\begin{minipage}{0.5\textwidth}
		\centering % must do this or your figure won't be centered
		\includegraphics[width=1\textwidth]{pure_recon.eps}
		\caption{Reconstructed box relative to ground truth} \label{pure_recon}
	\end{minipage}
\end{figure}

\noindent Across the four tested images (with no noise corruption) through 100 iterations of the algorithm, the average distance error was relatively small at $PRMSE = 0.2763 \quad$\ cm . 

\subsection{Corrupted Correspondence Points}
 Once the results of the algorithm are proven to work on its own data sets we observe the effect of the calibration and pose established by the algorithm on the pure data set (as opposed to the estimated) data to observe the distortions due to noise or error in correspondence. When applying normally distributed to each component of the correspondence points individually with a standard deviation of $\sigma = 100$ pixels we receive reconstruction results similar to those seen in \ref{noiseall}. From the calibration received, we test the ability to reconstruct the true (uncorrupted) data. This yields considerable disfigurement of the object. The reconstruction of the original box when corrupted by noise and the ground truth can seen in Figure \ref{noiseallrecon}.  To quantitatively capture the results of the noise reconstruction we compare the angles created between the correspondence points. Since the object is a box and the correspondence points lie along the edges, it is expected that all adjacent vectors are orthogonal to each other. However, in the presence of noise a representative data set of angles associated with the three visible faces of the box, and known correspondences appears, in Table \ref{angles}.
	\begin{center}
	\begin{tabular}[5pt]{| c| c|}
		\hline
		Point	& Angle(s) (degrees) \\[0.5ex] 
		\hline 	
		$1$& 103.6447 \\ \hline 
		$2$& 82.6421, 90.4116  \\ \hline 
		$3$& 88.9373  \\ \hline 
		$4$& 91.496, 92.3435  \\ \hline 
		$5$& 93.2852  \\ \hline 
		$6$& 85.5876, 76.0264  \\ \hline 
		$7$& 97.667, 88.7342, 89.1479  \\ \hline 
	\end{tabular}
	\captionof{table}{Object Dimensions}\label{angles}
\end{center}	
\begin{figure}[h]
	\centering % must do this or your figure won't be centered
	\captionsetup{justification=centering}
	\begin{minipage}{0.5\textwidth}
		\centering
		\includegraphics[width=1\textwidth]{noise_all.eps}
		\caption{Estimated image coordinates \newline(unique coordinate component noise)} \label{noiseall}
	\end{minipage}\hfill
	\begin{minipage}{0.5\textwidth}
		\centering % must do this or your figure won't be centered
		\includegraphics[width=1\textwidth]{noise_all_recon.eps}
		\caption{Reconstructed box relative to ground truth} \label{noiseallrecon}
	\end{minipage}
\end{figure}
As would be expected, the angles are centered about the appropriate mean value of $90$ degrees, however the deviations from precisely right angles by as much as $15.53$ degrees shows the obvious loss of the shape of the true object upon reconstruction. Similarly, the reliability of the reconstruction is tied to the amount of noise introduced in to the captured pixel coordinates. Under the condition of $\sigma = 100$ pixels, using 100 successive trials of the algorithm, there is an average PRMSE introduced of 1.65 cm, or an increase of error by approximately $595.4\%$. Through observation of various noise levels we observe that the RMS position error is (nearly) linear with the standard deviation of the noise (Figure \ref{errVstd}). It is essential to note that beyond a standard deviation of approximately 200 pixels it becomes common for pixels to be projected in to non-existent pixel mappings, and as such, are obviously invalid results. 
\begin{figure}[h]
	\centering % must do this or your figure won't be centered
	\includegraphics[width=.6\textwidth]{errVstd.eps}
	\caption{RMS Error as it varies with standard deviation of applied noise} \label{errVstd}
\end{figure}

\subsection{Improper Dimensions}
In order for the monocular pose algorithm to appropriately calibrate and successively reconstruct three dimensional objects it assumes the a priori knowledge of an object within the image's complete dimensions. To observe the effect of the error that is caused when incorrect dimensions are given to the algorithm, the true dimensions of the box were altered. This produced concerning results. By increasing the width of the box by a factor of 10 to 101, we are able to observe the pixel coordinate projection in Figure \ref{dimerr}. 
\begin{figure}[h]
	\centering % must do this or your figure won't be centered
	\captionsetup{justification=centering}
	\begin{minipage}{0.5\textwidth}
		\centering
		\includegraphics[width=1\textwidth]{dimerr.eps}
		\caption{Estimated image coordinates (inaccurate dimensions)} \label{dimerr}
	\end{minipage}\hfill
	\begin{minipage}{0.5\textwidth}
		\centering % must do this or your figure won't be centered
		\includegraphics[width=1\textwidth]{dimerr_recon.eps}
		\caption{Reconstructed box relative to ground truth (inaccurate dimensions)} \label{dimerrrecon}
	\end{minipage}
\end{figure}
Given only the image coordinate projections it could easily be assumed that the algorithm had appropriately reconstructed the box, despite the wrong dimensions. However, we then project these pixel coordinates and find that in the object frame, the computer believes that the reconstruction should match the dimensions that were provided by the user (see Figure \ref{dimerrrecon}). Numerical evidence lies within the translation vector, $T$, yielded by the monocular pose algorithm. For instance, using the initial given image (Figure \ref{noiseall}) it can be found that $T = [-21.8 \quad -33.8 \quad 90.0]^T$ cm which has a length of $98.61$ centimeters. In order for the algorithm to provide a reconstruction of the box where the width is falsely increased by a factor of ten that the translation vector is $T=[507.8 \quad -121.1 \quad 99.2]^T$ which has a length of $531.4 cm$. This implies that without the true box having been moved in the image, the translation vector implies that the origin of the box is an additional 4.32 meters farther away than it is in reality. As such, this algorithm, has potential for exceedingly poor calibration and misleading visual results in the event that objects being used to calibrate the system are not properly measured. 

\section{Conclusions}
In this project we have demonstrated the ability to compute the necessary information for the simultaneous calibration and pose estimation of a single monocular camera. The algorithm itself proved to be a fairly straightforward implementation, and in ideal or near ideal environments is able to provide reasonable estimates of the calibration of the camera, the object pose, and associated depths to the known correspondence points. However, this algorithm is not without its own problems. In the presence of high levels of noise corrupting the correspondence points a near linear increase in 3D position error is shown, and as a result the orthogonality of the box reconstruction is compromised beyond reasonable tolerances. Also, we have shown that in the event of inaccurate measurements of calibration objects being delivered to the algorithm that it is incapable of providing accurate scene reconstructions as well as yielding misleading images that are poorly indicative of the error that has occurred. 


\appendix % this command sets sectioning command to the appendix format
% uncomment the next line to start on a fresh page


\section{Additional Images}
\begin{figure}[h]
	\centering % must do this or your figure won't be centered
	\captionsetup{justification=centering}
	\begin{minipage}{0.5\textwidth}
		\centering % must do this or your figure won't be centered
		\includegraphics[width=1\textwidth]{im1.eps}
		\label{im1} 
	\end{minipage}\hfill
	\begin{minipage}{0.5\textwidth}
		\centering % must do this or your figure won't be centered
		\includegraphics[width=1\textwidth]{im2.eps}
		 \label{pure_recon}
	\end{minipage}
\end{figure}
\begin{figure}[h]
	\centering % must do this or your figure won't be centered
	\captionsetup{justification=centering}
	\begin{minipage}{0.5\textwidth}
		\centering % must do this or your figure won't be centered
		\includegraphics[width=1\textwidth]{im3.eps}
		 \label{pixcoords} 
	\end{minipage}\hfill
	\begin{minipage}{0.5\textwidth}
		\centering % must do this or your figure won't be centered
		\includegraphics[width=1\textwidth]{im4.eps}
		\label{pure_recon}
	\end{minipage}
	\caption{Four tested images.}
\end{figure}

\newpage
\section{Code Listings}\label{code}

% input the file containing the code
\lstinputlisting[caption={Top level implementation for Monocular Calibration and Pose Estimation},label={proj}]{MonoPose.m}
\lstinputlisting[caption={QR Decomp Based Algorithm Implementation},label={alg}]{MonoPoseQR.m} 
\lstinputlisting[caption={Rearranged QR Decomposition (Credit: Dr. John McInroy)},label={qrCom}]{qrCommute.m}


%% The commands below automatically generate the References section
%% using the ``sample_bib'' file I've given you.

%\newpage  % start a new page
%
%\bibliographystyle{ieeetr}
%\bibliography{a1_abbrv,Proj2}

\end{document} % always the last line of your document file
