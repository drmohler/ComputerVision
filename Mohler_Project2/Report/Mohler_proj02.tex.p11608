\documentclass[12pt]{article}
\usepackage{uwyo_report} % loads in all the specific formatting
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{float}
\usepackage{multicol}
\setlength{\multicolsep}{6.0pt plus 2.0pt minus 1.5pt}%


% if you want other packages, put them here

% macro for typesetting the word BibTeX
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
   T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}


\begin{document}

\title{EE 5450  Project 02: Convolution Neural Network Classification of the Animals Data Set}

\author{David R. Mohler}

%\date{December 7, 1941}

\maketitle

\section{Introduction} 
The rebirth of Neural Networks (NNs) and Machine Learning (ML) has offered revolutionary approaches to intelligent tasks previously unattainable in the field of image processing and computer vision. A major area of research continues to be the automated classification of images. Image classification through classical means of image processing tend to suffer due to problems with the wide variance in images to be classified and how their subject matter may be transformed from image to image. The discovery of Convolution Neural Networks (CNNs) lead to a massive increase in the accuracy of which images could be classified in an automated fashion. 

For this project we will demonstrate the development of several convolution neural networks that vary across a wide number of parameters for the purpose of classifying color images of three different types of animals. The CNN is tasked with classifying images of cats, dogs, and pandas, typical examples of images in the data set can be seen below. Through the variation of network parameters such as the number of layers and the amount of neurons within them, activation functions, learning rates, optimization routines, data augmentation, and other hyper parameters, we display the ability to increase the classification performance of the network. 
\begin{figure}[h]
	\centering % must do this or your figure won't be centered
	\captionsetup{justification=centering}
	\begin{minipage}{0.33\textwidth}
		\centering % must do this or your figure won't be centered
		\includegraphics[width=1\textwidth]{cats_00033.jpg}
	\end{minipage}\hfill
	\begin{minipage}{0.33\textwidth}
		\centering % must do this or your figure won't be centered
		\includegraphics[width=1\textwidth]{dogs_00013.jpg}
	\end{minipage}\hfill
	\begin{minipage}{0.33\textwidth}
		\centering % must do this or your figure won't be centered
		\includegraphics[width=1\textwidth]{panda_00082.jpg}
	\end{minipage}\hfill
\end{figure}

\section{Methods and Results}
After establishing an initial understanding of the basic \pc{ShallowNet} animal classification network that was provided, we begin with a relatively simple and small expansion to include fundamental elements of CNNs, once a working network is establish we are able to tune the parameters and observe the effects of varying the optimization techniques. Once a candidate optimizer has been selected we proceed to modify and tune the associated hyperparameters such as the learning rate (when applicable), batch size, number of training epochs, etc. Beyond this we also test the effects of data augmentation. 

\subsection{ShallowNet}
For baseline comparisons we will use the code provided by Dr. Rosebrock \cite{rosebrock} which is a simplistic implementation of a Convolution Neural Network designed with the goal of classification of images in to any of the three given animal categories. The basic structure of Rosebrock's network consists of:
\begin{center}
	\pc{INPUT => CONV => RELU => FC => SOFTMAX} \\
\end{center}
Even with the most basic of implementations, the network is able to provide a respectable first attempt at the classification of the images. As seen in Table \ref{SNR}, the network achieves an average accuracy $66\%$. Given that with three classes the network has a $33\%$ probability of randomly selecting the correct class, this is a decent first attempt. Using this as a lower bound we proceed to implement a number of other techniques that are common in CNN architectures in order to improve the network performance overall. 
\begin{figure}[h]
	\centering % must do this or your figure won't be centered
	\captionsetup{justification=centering}
	\begin{minipage}{0.5\textwidth}
		\centering % must do this or your figure won't be centered
		\includegraphics[width=1\textwidth]{BaselineResults_ShallowNet_opt-SGD.png}
		\caption{ShallowNet Accuracy} \label{SN}
	\end{minipage}\hfill
	\begin{minipage}{0.5\textwidth}
		\begin{center}
			\begin{tabular}[2pt]{| c| c| c| c|c|}
				\hline
				& Precision & Recall & F1 & Support \\[0.5ex] 
				\hline 	
				Cat   &    0.64	&0.57&	0.6&	262\\ \hline 
				Dog    &   0.57&	0.58&	0.58&	249\\ \hline 
				Panda   &   0.79&	0.86&	0.82&	239	\\ \hline 
				Avg/Tot  &     0.66 &	0.67 &	0.66 &	750\\ \hline 
				
			\end{tabular}
			\captionof{table}{ShallowNet Results}\label{SNR}
		\end{center}	
	\end{minipage}
\end{figure}
\newpage
\subsection{MohlerNet2}

The initial modification to the original network lies in the expansion of the architecture to incorporate key elements of CNNs, such as pooling layers and incorporation of neuron dropout prior to the fully connected layer. The general architecture of \textbf{``\pc{MohlerNet2}''} is as follows: 
\begin{center}
	\pc{INPUT => CONV => CONV => MAXPOOL => DROPOUT(0.5) => FC => SOFTMAX} \\
\end{center}
More specifically, the CNN employs the use of $3\times3$ kernels in the convolution layers, where the first layer generates 32 feature maps and the second expands to 64 feature maps prior to applying a $2\times2$ max-pooling to the output. From the pooled output a neuron dropout probability of $50\%$ is applied in order to attempt to reduce neuron dependencies and memorization of data. Using the classical stochastic gradient descent (SGD) approach to the optimization of the network we were able to see an immediate and reproducible increase in the ability of the network to accurately classify the three categories of animal images.  From Table \ref{MNR}, it can be seen that the inclusion of the additional convolutional layer, pooling, and dropout was able to provide a $6\%$ increase in the accuracy of the network. For this experiment we used the keras recommended learning rate ($LR = 0.01$). As a more objective measure of the algorithm we can view the F-1 score for each tested network. The F-1 score captures information regarding the false positives and false negatives in the classification process, this gives a more objective view from algorithm to algorithm . From this we can also see a similar increase in F-1 score between the original ShallowNet implementation and the first implementation of MohlerNet (MohlerNet2). 

\begin{figure}[h]
	\centering % must do this or your figure won't be centered
	\captionsetup{justification=centering}
	\begin{minipage}{0.5\textwidth}
		\centering % must do this or your figure won't be centered
		\includegraphics[width=1\textwidth]{MohlerNet2_opt-SGD_KEEP.png}
		\caption{MohlerNet2 Accuracy, Learning Rate:0.01} \label{MN2}
	\end{minipage}\hfill
	\begin{minipage}{0.5\textwidth}
		\begin{center}
			\begin{tabular}[5pt]{| c| c| c| c|c|}
				\hline
					& Precision & Recall & F1 & Support \\[0.5ex] 
				\hline 	
				 Cat   &    0.67  &    0.75   &   0.71   &    262\\ \hline 
				 Dog    &   0.64    &  0.60   &   0.62   &    249\\ \hline 
				 Panda   &    0.88   &  0.82 &     0.85  &     239\\ \hline 
				 Avg/Tot  &     0.72    &  0.72 &     0.72    &   750\\ \hline 

			\end{tabular}
			\captionof{table}{MohlerNet2 Classification Results}\label{MNR}
		\end{center}	
	\end{minipage}
\end{figure}

Once the first trials of the SGD optimized results were obtained we observed the effect of modifying the optimizer for the network. While using the AdaGrad and Adam optimizers, which apply adaptive learning rates, we saw no improvement in any of the scoring metrics. Actually, there was a reduction in accuracy from the original SGD approach, down to an average of approximately $70\%$ classification accuracy. In order to attempt to increase the accuracy of the network we must take in to account that we are dealing with a relatively small data set (by modern standards). With the small number of images being used to train the network there is a high probability that we will experience over-fitting while attempting to increase performance. In order to counter this, we next applied data augmentation to the training set. This allows us to modify the images in moderate ways (rotations, flips, sheering, etc.) such that they are presented to the network in a new way each epoch. The inclusion of data augmentation yielded another (relatively) considerable increase in accuracy and F1-Score. As shown in Table \ref{MN2RAug}, both metrics increased by $4\%$ such that the MohlerNet2 implementation could classify with $76\%$ accuracy. However, from Figure \ref{MN2Aug} it can be seen that from the gap between training accuracy and validation accuracy, the network is experiencing some mild over fitting, which indicates that some modifications may be able to improve the performance further, these remain to be discovered.

\begin{figure}[h]
	\centering % must do this or your figure won't be centered
	\captionsetup{justification=centering}
	\begin{minipage}{0.5\textwidth}
		\centering % must do this or your figure won't be centered
		\includegraphics[width=1\textwidth]{MohlerNet2_opt-AdaGradAugment_KEEP.png}
		\caption{MohlerNet2 with Data Augmentation Accuracy, AdaGrad Optimized} \label{MN2Aug}
	\end{minipage}\hfill
	\begin{minipage}{0.5\textwidth}
		\begin{center}
			\begin{tabular}[5pt]{| c| c| c| c|c|}
				\hline
				& Precision & Recall & F1 & Support \\[0.5ex] 
				\hline 	
				Cat   &    0.75&	0.66&	0.71&	262\\ \hline 
				Dog    &   0.69&	0.7&	0.69&	249\\ \hline 
				Panda   &   0.84&	0.93&	0.89&	239\\ \hline 
				Avg/Tot  &    0.76&	0.76&	0.76&	750\\ \hline 
				
			\end{tabular}
			\captionof{table}{MohlerNet2 with Data Augmentation Results}\label{MN2RAug}
		\end{center}	
	\end{minipage}
\end{figure}

\subsection{MohlerNet3}
The next generation implementation of MohlerNet (MohlerNet3) is based upon a simplified version of the layer architecture applied by Alex-Net \cite{NIPS2012_4824}. This iteration has a large increase in layers and number of computations relative to its predecessor. As found with the previous iteration, when the network is trained without the application of data augmentation there is a loss of performance. MohlerNet3 obtained an accuracy of $71\%$, and as such its complete results have not been included here. However, when the data is augmented the accuracy of the network is boosted to a value of $77\%$, however, the F1 and recall scores took a dip, which indicates this network is more subject to false positives and false negatives. 

\begin{center}
	\pc{INPUT => [CONV => MAXPOOL]*2 => CONV*3 => MAXPOOL => DROPOUT(0.5) => FC => SOFTMAX} \\
\end{center}


\begin{figure}[h]
	\centering % must do this or your figure won't be centered
	\captionsetup{justification=centering}
	\begin{minipage}{0.5\textwidth}
		\centering % must do this or your figure won't be centered
		\includegraphics[width=1\textwidth]{MohlerNet3_opt-AdamaxAugmented_KEEP.png}
		\caption{MohlerNet3 with Data Augmentation Accuracy, AdaMax Optimized} \label{MN3Aug}
	\end{minipage}\hfill
	\begin{minipage}{0.5\textwidth}
		\begin{center}
			\begin{tabular}[5pt]{| c| c| c| c|c|}
				\hline
				& Precision & Recall & F1 & Support \\[0.5ex] 
				\hline 	
				Cat   &    0.81&      0.58 &     0.68  &     262\\ \hline 
				Dog    &   0.62 &     0.78 &     0.69 &      249    \\ \hline 
				Panda   &   0.88&      0.91  &    0.89  &     239   \\ \hline 
				Avg/Tot  &    0.77 &     0.75   &   0.75   &    750\\ \hline 
				
			\end{tabular}
			\captionof{table}{MohlerNet3 with Data Augmentation Results}\label{MN3RAug}
		\end{center}	
	\end{minipage}
\end{figure}

\subsection{MohlerNet4}

Applying what has been descovered in the first two editions of MohlerNet, we modifiy the architecture of the network into a medium sized network (relative to the first two) which balances a low number of layers with the aim of the higher accuracy provided by the larger of the two networks. From this we select the AdaMax optimizer once more and operate under the assumption that the augmentation of the data consistently provides a boost in performance. Using the architecture shown below, we use 32 activation maps in the first two convolution layers with $3\times3$ kernels and $2\times2$ max-pooling, and 64 activation maps in the final convolution layer. However, in this iteration we add an additional fully connected layer prior to the output layer with 64 fully connected neurons. Using the ReLU activation for all layers we achieve results that are comparable to the much larger MohlerNet3 as shown in Table \ref{MN4RAug}

\begin{center}
	\pc{INPUT => [CONV => MAXPOOL]*3 => FC => DROPOUT => FC => SOFTMAX} \\
\end{center}


\begin{figure}[h]
	\centering % must do this or your figure won't be centered
	\captionsetup{justification=centering}
	\begin{minipage}{0.5\textwidth}
		\centering % must do this or your figure won't be centered
		\includegraphics[width=1\textwidth]{MohlerNet4_opt-Adamaxtests.png}
		\caption{MohlerNet4 with Data Augmentation Accuracy, AdaMax Optimized} \label{MN4Aug}
	\end{minipage}\hfill
	\begin{minipage}{0.5\textwidth}
		\begin{center}
			\begin{tabular}[5pt]{| c| c| c| c|c|}
				\hline
				& Precision & Recall & F1 & Support \\[0.5ex] 
				\hline 	
				Cat   &    0.76&	0.61&	0.67&	262\\ \hline 
				Dog    &   0.64&	0.79&	0.71&	249    \\ \hline 
				Panda   &  0.93&	0.92&	0.92&	239  \\ \hline 
				Avg/Tot  &   0.77&	0.77&	0.76&	750\\ \hline 
				
			\end{tabular}
			\captionof{table}{MohlerNet4 with Data Augmentation Results}\label{MN4RAug}
		\end{center}	
	\end{minipage}
\end{figure}

With the promising results delivered by MohlerNet4, we next experimented with the modification of the activation function in the first dense layer. Instead of the use of ReLU as is done in all previous networks and layers, we attempted the use of the hyperbolic tangent activation function. In terms of classification accuracy, this yielded the best network. Edging out the previous best by a single percentage point, we were able to obtain a classification percentage of $78\%$. While this result was able to be replicated to within a single percent, it should be noted that the loss and accuracy characteristics from Figure \ref{MN4Augtanh} are not typical. It can be seen that the validation accuracy out strips the training accuracy by nearly $20\%$, as well as displaying the training loss not approaching the desired value of zero within the trained period. 

\begin{figure}[h]
	\centering % must do this or your figure won't be centered
	\captionsetup{justification=centering}
	\begin{minipage}{0.5\textwidth}
		\centering % must do this or your figure won't be centered
		\includegraphics[width=1\textwidth]{MohlerNet4_opt-Adamax.png}
		\caption{MohlerNet4 with Data Augmentation Accuracy, AdaMax Optimized} \label{MN4Augtanh}
	\end{minipage}\hfill
	\begin{minipage}{0.5\textwidth}
		\begin{center}
			\begin{tabular}[5pt]{| c| c| c| c|c|}
				\hline
				& Precision & Recall & F1 & Support \\[0.5ex] 
				\hline 	
				Cat   &    0.81&      0.58 &     0.68  &     262\\ \hline 
				Dog    &   0.62 &     0.78 &     0.69 &      249    \\ \hline 
				Panda   &   0.88&      0.91  &    0.89  &     239   \\ \hline 
				Avg/Tot  &    0.77 &     0.75   &   0.75   &    750\\ \hline 
				
			\end{tabular}
			\captionof{table}{MohlerNet4 with Data Augmentation (tanh variant) Results}\label{MN4RAugtanh}
		\end{center}	
	\end{minipage}
\end{figure}



\section{Conclusions}

\newpage
\appendix % this command sets sectioning command to the appendix format
% uncomment the next line to start on a fresh page



\section{Code Listings}\label{code}

% input the file containing the code
\lstinputlisting[caption={MohlerNet Implementations},label={MNET}]{MohlerNet.py}
%\lstinputlisting[caption={QR Decomp Based Algorithm Implementation},label={alg}]{MonoPoseQR.m} 
%\lstinputlisting[caption={Rearranged QR Decomposition (Credit: Dr. John McInroy)},label={qrCom}]{qrCommute.m}


%% The commands below automatically generate the References section
%% using the ``sample_bib'' file I've given you.

\newpage  % start a new page
\bibliographystyle{ieeetr}
\bibliography{Proj2}

\end{document} % always the last line of your document file
